Atemp <- conf_mat(table(testSet[, 9], SVMPredict))
Btemp <- conf_mat(table(testSet[, 9], RFPredict))
A <- autoplot(Atemp, type = "heatmap") + scale_fill_gradient(low = "#6c8ead", high = "#bf1a2f") + xlab("Predicted Values") + ylab("Actual Values")
B <- autoplot(Btemp, type = "heatmap") + scale_fill_gradient(low = "#6c8ead", high = "#bf1a2f") + xlab("Predicted Values") + ylab("Actual Values")
ggarrange(A, B, labels = c("SVM", "RF"))
1 - mean(SVMPredict == testSet$class)
1 - mean(RFPredict == testSet$class)
require(caret)
combTestTain <- rbind(trainSet, testSet)
train_control <- trainControl(method = "cv", number = 10)
RFcvModel <- train(factor(class) ~ ., data = combTestTain, method = "rf", trControl = train_control)
print(RFcvModel)
RFcvModel$resampledCM
install.packages("pROC")
install.packages("pROC")
library(pROC)
require(e1071)
## NBClass <-
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[-9], type = "response")
NBClass
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[-9], type = "response")
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[-9], type = "response")
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, factor(testSet[-9]), type = "response")
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, factor(testSet[-9]), type = "response")
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, (testSet[1:8], type = "response")
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, (testSet[1:8], type = "response")
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[1:8], type = "response")
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[1:8], type = "response")
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[-9], type = "raw")
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[-9], type = "raw")
1 - mean(NBPredict == testSet$class)
NBPredict
NBYPred <- ifelse(NBPredict > 0.5, "Positive", "Negative")
1 - mean(NBPredict == testSet$class)
NBYPred <- ifelse(NBPredict > 0.5, "Positive", "Negative")
1 - mean(NBPredict == testSet$class)
NBYPred <- ifelse(NBPredict > 0.5, "Positive", "Negative")
1 - mean(NBPredict == testSet$class)
NBYPred <- ifelse(NBPredict > 0.5, "Positive", "Negative")
1 - mean(NBYPred == testSet$class)
NBYPred <- ifelse(NBPredict > 0.5, "Positive", "Negative")
1 - mean(NBYPred == testSet$class)
NBYPred <- ifelse(NBPredict > 0.5, "Positive", "Negative")
1 - mean(NBYPred == testSet$class)
NBYPred <- ifelse(NBPredict > 0.5, "Positive", "Negative")
1 - mean(NBYPred == testSet$class)
NBYPred <- ifelse(NBPredict > 0.5, "Positive", "Negative")
1 - mean(NBYPred == testSet$class)
require(caret)
combTestTain <- rbind(trainSet, testSet)
train_control <- trainControl(method = "cv", number = 10)
NBcvModel <- train(factor(class) ~ ., data = combTestTain, method = "nb", trControl = train_control)
print(NBcvModel)
RFcvModel$resampledCM
library(knitr)
library(rmdformats)
library(ggplot2)
library(reticulate)
library(reshape2)
library(ggpubr)
library(kableExtra)
library(tidyverse)
library(foreign)
library(dplyr)
library(vcd)
library(VGAM)
library(nnet)
library(dplyr)
use_python("C:/Users/Collin/anaconda3/python.exe")
opts_chunk$set(echo=FALSE,
cache=TRUE,
prompt=FALSE,
tidy=TRUE,
comment=NA,
message=FALSE,
warning=FALSE)
opts_knit$set(width=75)
options(max.print="75")
py_run_string('import os')
py_run_string("os.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = 'C:/Users/Collin/anaconda3/Library/plugins/platforms'")
setwd("C:/Users/Collin/Desktop/Academic/Courses/DSCI 610/Final Project")
dataset <- read.csv("diabetes_data_upload.csv")
dataset <- mutate(dataset, Age.Group = case_when(Age <= 35 ~ "Young",
between(Age, 36, 45) ~ "Mid-Young",
between(Age, 46, 55) ~ "MidAge",
between(Age, 56, 65) ~ "Mid-Old",
Age >= 66 ~ "Old"))
dataset <- dataset[c("Gender", "Polyuria", "Polydipsia", "sudden.weight.loss", "weakness", "Polyphagia", "Genital.thrush",
"visual.blurring", "Itching", "Irritability", "delayed.healing", "partial.paresis", "muscle.stiffness",
"Alopecia", "Obesity", "Age.Group", "class")]
for (i in 1:16) {
resultstemp <- as.vector(chisq.test(dataset$class, dataset[, i]))
print(paste(names(dataset[i]), resultstemp$p.value))
}
require(FactoMineR)
require(factoextra)
res.mca <- MCA(dataset[1:16], graph = F)
fviz_mca_var(res.mca, col.var = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE, # Avoid text overlapping
ggtheme = theme_minimal())
fviz_screeplot(res.mca, addlabels = TRUE, ylim = c(0, 45))
fviz_mca_var(res.mca, choice = "mca.cor",
repel = T,
ggtheme = theme_classic())
variables <- get_mca_var(res.mca)
require(factoextra)
fviz_cos2(res.mca, choice = "var", axes = 1:4)
head(round(variables$cos2, 2), 4)
require(ggcorrplot)
ggcorrplot(variables$cos2)
fviz_mca_var(res.mca, col.var = "cos2",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = TRUE, # Avoid text overlapping
ggtheme = theme_minimal())
require(caTools)
set.seed(101)
sample <- sample.split(dataset$class, SplitRatio = 0.7)
train <- subset(dataset, sample == TRUE)
test <- subset(dataset, sample == FALSE)
trainSet <-select(train, Polydipsia, Polyphagia, partial.paresis, Polyuria, visual.blurring, delayed.healing, muscle.stiffness, Gender, class)
testSet <-  select(test, Polydipsia, Polyphagia, partial.paresis, Polyuria, visual.blurring, delayed.healing, muscle.stiffness, Gender, class)
logReg <- glm(factor(class) ~ factor(Polydipsia)  + factor(Polyuria) + factor(visual.blurring) + factor(Polyphagia) + factor(partial.paresis) + factor(delayed.healing) + factor(muscle.stiffness) + factor(Gender) , family = binomial, data = trainSet)
logPredict <- predict(logReg, testSet[-9], type = "response")
require(e1071)
SVMClass <- svm(factor(class) ~ factor(Polydipsia)  + factor(Polyuria) + factor(visual.blurring) + factor(Polyphagia) + factor(partial.paresis) + factor(delayed.healing) + factor(muscle.stiffness) + factor(Gender), kernel = "linear", data = trainSet)
SVMPredict <- predict(SVMClass, testSet[-9], type = "response")
require(randomForest)
RFClass <- randomForest(x = trainSet[-9], y = factor(trainSet$class), ntree = 20)
RFPredict <- predict(RFClass, testSet[-9], type = "response")
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[-9], type = "raw")
require(ggpubr)
require(yardstick)
Atemp <- conf_mat(table(testSet[, 9], SVMPredict))
Btemp <- conf_mat(table(testSet[, 9], RFPredict))
A <- autoplot(Atemp, type = "heatmap") + scale_fill_gradient(low = "#6c8ead", high = "#bf1a2f") + xlab("Predicted Values") + ylab("Actual Values")
B <- autoplot(Btemp, type = "heatmap") + scale_fill_gradient(low = "#6c8ead", high = "#bf1a2f") + xlab("Predicted Values") + ylab("Actual Values")
ggarrange(A, B, labels = c("SVM", "RF"))
logYPred <- ifelse(logPredict > 0.5, "Positive", "Negative")
1 - mean(logYPred == testSet$class)
1 - mean(SVMPredict == testSet$class)
1 - mean(RFPredict == testSet$class)
NBYPred <- ifelse(NBPredict > 0.5, "Positive", "Negative")
1 - mean(NBYPred == testSet$class)
require(caret)
combTestTain <- rbind(trainSet, testSet)
train_control <- trainControl(method = "cv", number = 10)
RFcvModel <- train(factor(class) ~ ., data = combTestTain, method = "rf", trControl = train_control)
print(RFcvModel)
RFcvModel$resampledCM
require(caret)
combTestTain <- rbind(trainSet, testSet)
train_control <- trainControl(method = "cv", number = 10)
NBcvModel <- train(factor(class) ~ ., data = combTestTain, method = "nb", trControl = train_control)
print(NBcvModel)
require(pROC)
require(pROC)
require(pROC)
logROC <- multiclass.roc(factor(class) ~ factor(Polydipsia)  + factor(Polyuria) + factor(visual.blurring) + factor(Polyphagia) + factor(partial.paresis) + factor(delayed.healing) + factor(muscle.stiffness) + factor(Gender) , family = binomial, data = trainSet)
require(pROC)
logROC <- multiclass.roc(testSet$class, logYPred)
require(pROC)
logROC <- multiclass.roc(testSet$class, logYPred, levels = c("Positive", "Negative"))
require(pROC)
logROC <- multiclass.roc(testSet$class, logYPred, levels = c("Positive", "Negative"))
require(pROC)
logROC <- multiclass.roc(factor(testSet$class), factor(logYPred), levels = c("Positive", "Negative"))
require(pROC)
logROC <- multiclass.roc(factor(testSet$class), factor(logYPred), levels = c(0, 1))
require(pROC)
logROC <- multiclass.roc(factor(testSet$class), factor(logYPred), levels = c(0, 1))
require(pROC)
RFRoc <- multiclass.roc(testSet$class, RFPredict)
require(pROC)
RFRoc <- multiclass.roc(testSet$class, RFPredict)
require(pROC)
RFRoc <- multiclass.roc(factor(testSet$class), factor(RFPredict))
require(pROC)
RFRoc <- multiclass.roc(factor(testSet$class), factor(RFPredict))
1 - mean(RFPredict == testSet$class)
1 - mean(RFPredict == testSet$class)
1 - mean(SVMPredict == testSet$class)
logYPred <- ifelse(logPredict > 0.5, "Positive", "Negative")
1 - mean(logYPred == testSet$class)
require(ggpubr)
require(yardstick)
Atemp <- conf_mat(table(testSet[, 9], SVMPredict))
Btemp <- conf_mat(table(testSet[, 9], RFPredict))
Ctemp <- conf_mat(table(testSet[, 9], logYPred))
Dtemp <-conf_mat(table(testSet[, 9], NBYPred))
require(ggpubr)
require(yardstick)
Atemp <- conf_mat(table(testSet[, 9], SVMPredict))
Btemp <- conf_mat(table(testSet[, 9], RFPredict))
Ctemp <- conf_mat(table(testSet[, 9], logYPred))
Dtemp <- conf_mat(table(testSet[, 9], NBYPred))
length(NBYPred)
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[-9], type = "raw")
length(RFPredict)
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[-9], type = "raw")
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[-9], type = "raw")
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[-9], type = "raw")
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[-9], type = "raw")
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[-9], type = "raw")
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[-9], type = "raw")
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[-9], type = "raw")
NBPredict
NBYPred <- ifelse(NBPredict[1] > 0.5, "Positive", "Negative")
1 - mean(NBYPred == testSet$class)
NBYPred <- ifelse(NBPredict[1] > 0.5, "Positive", "Negative")
1 - mean(NBYPred == testSet$class)
NBYPred <- ifelse(NBPredict[1] > 0.5, "Positive", "Negative")
1 - mean(NBYPred == testSet$class)
NBYPred <- ifelse(NBPredict[2] > 0.5, "Positive", "Negative")
1 - mean(NBYPred == testSet$class)
NBYPred <- ifelse(NBPredict[2] > 0.5, "Positive", "Negative")
1 - mean(NBYPred == testSet$class)
NBYPred <- ifelse(NBPredict[2] > 0.5, "Positive", "Negative")
1 - mean(NBYPred == testSet$class)
NBYPred <- ifelse(NBPredict[2] > 0.5, "Positive", "Negative")
1 - mean(NBYPred == testSet$class)
NBYPred <- ifelse(NBPredict > 0.5, "Positive", "Negative")
1 - mean(NBYPred == testSet$class)
NBYPred <- ifelse(NBPredict > 0.5, "Positive", "Negative")
1 - mean(NBYPred == testSet$class)
NBPredict
NBYPred
NBYPred <- ifelse(NBPredict > 0.5, "Positive", "Negative")
1 - mean(NBYPred == testSet$class)
NBYPred <- ifelse(NBPredict > 0.5, "Positive", "Negative")
1 - mean(NBYPred == testSet$class)
hist(NBPredict[1])
NBClass
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[-9], type = "raw")
table(NBPredict, testSet$class)
NBClass
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[-9])
table(NBPredict, testSet$class)
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[-9])
table(NBPredict, testSet$class)
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[-9])
table(NBPredict, testSet$class)
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[-9])
table(NBPredict, testSet$class)
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[-9])
table(NBPredict, testSet$class)
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[-9])
table(NBPredict, testSet$class)
NBYPred <- ifelse(NBPredict > 0.5, "Positive", "Negative")
1 - mean(NBYPred == testSet$class)
NBPredict
1 - mean(NBPredict == testSet$class)
1 - mean(NBPredict == testSet$class)
1 - mean(NBPredict == testSet$class)
require(ggpubr)
require(yardstick)
Atemp <- conf_mat(table(testSet[, 9], SVMPredict))
Btemp <- conf_mat(table(testSet[, 9], RFPredict))
Ctemp <- conf_mat(table(testSet[, 9], logYPred))
Dtemp <- conf_mat(table(testSet[, 9], NBPredict))
A <- autoplot(Atemp, type = "heatmap") + scale_fill_gradient(low = "#6c8ead", high = "#bf1a2f") + xlab("Predicted Values") + ylab("Actual Values")
B <- autoplot(Btemp, type = "heatmap") + scale_fill_gradient(low = "#6c8ead", high = "#bf1a2f") + xlab("Predicted Values") + ylab("Actual Values")
C <- autoplot(Ctemp, type = "heatmap") + scale_fill_gradient(low = "#6c8ead", high = "#bf1a2f") + xlab("Predicted Values") + ylab("Actual Values")
D <- autoplot(Dtemp, type = "heatmap") + scale_fill_gradient(low = "#6c8ead", high = "#bf1a2f") + xlab("Predicted Values") + ylab("Actual Values")
ggarrange(A, B, C, D, labels = c("SVM", "RF", "logReg", "NB"), nrow = 2, ncol = 2)
require(ROCR)
require(ROCR)
perf1 <- performance(SVMPredict, "tpr", "rpp")
install.packages("precrec")
require(precrec)
mod1 <- evalmod(scores = SVMPredict, labels = testSet$class)
require(precrec)
mod1 <- evalmod(scores = SVMPredict, labels = testSet$class)
require(ggpubr)
require(yardstick)
require(ggpubr)
require(yardstick)
Atemp <- conf_mat(table(testSet[, 9], SVMPredict))
Btemp <- conf_mat(table(testSet[, 9], RFPredict))
Ctemp <- conf_mat(table(testSet[, 9], logYPred))
Dtemp <- conf_mat(table(testSet[, 9], NBPredict))
A <- autoplot(Atemp, type = "heatmap") + scale_fill_gradient(low = "#6c8ead", high = "#bf1a2f") + xlab("Predicted Values") + ylab("Actual Values")
B <- autoplot(Btemp, type = "heatmap") + scale_fill_gradient(low = "#6c8ead", high = "#bf1a2f") + xlab("Predicted Values") + ylab("Actual Values")
C <- autoplot(Ctemp, type = "heatmap") + scale_fill_gradient(low = "#6c8ead", high = "#bf1a2f") + xlab("Predicted Values") + ylab("Actual Values")
D <- autoplot(Dtemp, type = "heatmap") + scale_fill_gradient(low = "#6c8ead", high = "#bf1a2f") + xlab("Predicted Values") + ylab("Actual Values")
ggarrange(A, B, C, D, labels = c("SVM", "RF", "logReg", "NB"), nrow = 2, ncol = 2)
require(e1071)
NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
NBPredict <- predict(NBClass, testSet[-9])
logYPred <- ifelse(logPredict > 0.5, "Positive", "Negative")
logRegError <- 1 - mean(logYPred == testSet$class)
SVMError <- 1 - mean(SVMPredict == testSet$class)
RFError <- 1 - mean(RFPredict == testSet$class)
NBError <- 1 - mean(NBPredict == testSet$class)
testErrorDf <- data.frame(Test <- c("LogReg", "SVM", "RF", "NB"), Values <- c(logRegError, SVMError, RFError, NBError))
kbl(testErrorDf, caption = "Each Machine learning algorithm and the respective test error for each.",
col.names = c("Test", "Test Error"))
rmarkdown::render("FinalArticle.Rmd", output_format = "pdf_document")
setwd("C:/Users/Collin/Desktop/Academic/Courses/DSCI 610/FinalArticle")
rmarkdown::render("FinalArticle.Rmd", output_format = "pdf_document")
require(reticulate)
use_python("C:/Users/Collin/anaconda3/python.exe")
py_run_string('import os')
py_run_string("os.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = 'C:/Users/Collin/anaconda3/Library/plugins/platforms'")
# require(dplyr)
# dataset <- read.csv("diabetes_data_upload.csv")
#
# dataset <- mutate(dataset, Age.Group = case_when(Age <= 35 ~ "Young",
#                                                  between(Age, 36, 45) ~ "Mid-Young",
#                                                  between(Age, 46, 55) ~ "MidAge",
#                                                  between(Age, 56, 65) ~ "Mid-Old",
#                                                  Age >= 66 ~ "Old"))
# dataset <- dataset[c("Gender", "Polyuria", "Polydipsia", "sudden.weight.loss",
# "weakness", "Polyphagia", "Genital.thrush",
# "visual.blurring", "Itching", "Irritability",
# "delayed.healing", "partial.paresis", "muscle.stiffness",
# "Alopecia", "Obesity", "Age.Group", "class")]
#
#
# require(FactoMineR)
# require(factoextra)
#
#
#
#
# res.mca <- MCA(dataset[1:16], graph = F)
#
# fviz_mca_var(res.mca, col.var = "cos2",
#              gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
#              repel = TRUE, # Avoid text overlapping
#              ggtheme = theme_minimal())
#
# require(caTools)
#
# set.seed(101)
#
# sample <- sample.split(dataset$class, SplitRatio = 0.7)
#
# train <- subset(dataset, sample == TRUE)
#
# test <- subset(dataset, sample == FALSE)
#
#
# trainSet <-select(train, Polydipsia, Polyphagia, partial.paresis, Polyuria,
# visual.blurring, delayed.healing, muscle.stiffness, Gender, class)
#
# testSet <-  select(test, Polydipsia, Polyphagia, partial.paresis, Polyuria,
# visual.blurring, delayed.healing, muscle.stiffness, Gender, class)
#
# descrData <- data.frame("Training" = length(trainSet[, 1]),
# "Testing" = length(testSet[, 1]))
#
# require(kableExtra)
# kbl(descrData, caption = "Length of the training set and testing set.")
#
# logReg <- glm(factor(class) ~ factor(Polydipsia)  + factor(Polyuria) +
# factor(visual.blurring)
# + factor(Polyphagia) + factor(partial.paresis) +
# factor(delayed.healing) +
#   factor(muscle.stiffness) + factor(Gender) ,
# family = binomial, data = trainSet)
#
#
#
# logPredict <- predict(logReg, testSet[-9], type = "response")
#
# hist(logPredict, main = NULL, xlab = "Probability", col = "#264653")
#
# require(e1071)
#
# SVMClass <- svm(factor(class) ~ factor(Polydipsia)  + factor(Polyuria) +
# factor(visual.blurring) + factor(Polyphagia) + factor(partial.paresis) +
# factor(delayed.healing) +    factor(muscle.stiffness) + factor(Gender),
# kernel = "linear", data = trainSet)
#
# SVMPredict <- predict(SVMClass, testSet[-9], type = "response")
#
# require(randomForest)
#
# RFClass <- randomForest(x = trainSet[-9], y = factor(trainSet$class), ntree = 20)
#
# RFPredict <- predict(RFClass, testSet[-9], type = "response")
# require(e1071)
#
#
#
# NBClass <- naiveBayes(x = trainSet[-9], y = factor(trainSet$class))
#
# NBPredict <- predict(NBClass, testSet[-9])
#
# logYPred <- ifelse(logPredict > 0.5, "Positive", "Negative")
#
# logRegError <- 1 - mean(logYPred == testSet$class)
#
# SVMError <- 1 - mean(SVMPredict == testSet$class)
#
# RFError <- 1 - mean(RFPredict == testSet$class)
# NBError <- 1 - mean(NBPredict == testSet$class)
#
# require(ggpubr)
# require(yardstick)
#
#
# Atemp <- conf_mat(table(testSet[, 9], SVMPredict))
#
# Btemp <- conf_mat(table(testSet[, 9], RFPredict))
#
# Ctemp <- conf_mat(table(testSet[, 9], logYPred))
#
# Dtemp <- conf_mat(table(testSet[, 9], NBPredict))
#
#
# A <- autoplot(Atemp, type = "heatmap") + scale_fill_gradient(low = "#6c8ead",
#                                                              high = "#bf1a2f")+
# xlab("Predicted Values")
# + ylab("Actual Values")
#
# B <- autoplot(Btemp, type = "heatmap") + scale_fill_gradient(low = "#6c8ead",
#                                                              high = "#bf1a2f") +
# xlab("Predicted Values")
# + ylab("Actual Values")
#
# C <- autoplot(Ctemp, type = "heatmap") + scale_fill_gradient(low = "#6c8ead",
#                                                              high = "#bf1a2f") +
# xlab("Predicted Values")
# + ylab("Actual Values")
#
# D <- autoplot(Dtemp, type = "heatmap") + scale_fill_gradient(low = "#6c8ead",
#                                                              high = "#bf1a2f") +
# xlab("Predicted Values")
# + ylab("Actual Values")
#
#
# ggarrange(A, B, C, D, labels = c("SVM", "RF", "logReg", "NB"), nrow = 2, ncol = 2)
#
# ggarrange(A, B, labels = c("SVM", "RF"))
#
# testErrorDf <- data.frame(Test <- c("LogReg", "SVM", "RF"),
# Values <- c(logRegError, SVMError, RFError, NBError))
# kbl(testErrorDf,
# caption = "Each Machine learning algorithm and the respective test error for each.",
#       col.names = c("Test", "Test Error"))
#
# require(caret)
#
# combTestTain <- rbind(trainSet, testSet)
#
#
# train_control <- trainControl(method = "cv", number = 10)
#
# RFcvModel <- train(factor(class) ~ ., data = combTestTain, method = "rf",
# trControl = train_control)
#
# kbl(data.frame(Accuracy = mean(RFcvModel$results[, 2])),
# caption = "Accuracy score from the 10-fold cross-validation RF classification.")
rmarkdown::render("FinalArticle.Rmd", output_format = "pdf_document")
